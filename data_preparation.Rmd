---
title: "COVID-19 data preparation"
author: "Sven Lautenbach"
date: "7/26/2021"
output: html_document
  highlight: pygments
  theme: sandstone
editor_options: 
  chunk_output_type: console
---

In this document we guide you through all steps to get ready for a spatial analysis of COVID 19 incidence rates in Germany. This will include data acquisition, checks and validation, modification and aggregation. The integration of a spatial data set will be particularly relevant here.

## Libraries

In the following libraries are loaded whose functions are mainly important for 4 areas: Tabular data, time series, spatial data models + statistics and visualization.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse) # tidy workflows, data wrangling
require(ggplot2) # viz plots
require(lubridate) # date time
require(sf) # spatial data
require(zoo) # timeseries
require(spdep) # spatial stats
require(tmap) # viz maps
require(tidyquant) # quantitative financal analysis
require(DataExplorer) # Explorative data analysis,  devtools::install_github("boxuancui/DataExplorer")
#require(ows4R) # handling OGC webservices
```

# Data acquisition

-   COVID 19 case data by RKI, manual download and metadata here: <https://www.arcgis.com/home/item.html?id=f10774f1c63e40168479a1feb6c7ca74>

-   Kreis (counties or districts) boundaries with population counts, manual download and metadata here:\
    <https://gdz.bkg.bund.de/index.php/default/verwaltungsgebiete-1-250-000-mit-einwohnerzahlen-ebenen-stand-31-12-vg250-ew-ebenen-31-12.html>

The following code-chunk will check if the data is already downloaded and for the COVID cases data check if it's up-to-date.

```{r}
# create folder structure
if (!dir.exists("data/")) {
  dir.create("data/", recursive = T)
}

options(timeout = 1080) # avoid download cut-off at 60sec

# check if RKI case data is downloaded/the latest state
if (as.Date(Sys.time()) - as.Date(file.info("data/RKI_COVID19.csv")$ctime) > 0) {
  download.file(
    "https://www.arcgis.com/sharing/rest/content/items/f10774f1c63e40168479a1feb6c7ca74/data",
    destfile = "data/RKI_COVID19.csv",
    mode = 'wb'
  )
}

# check if Kreis boundaries are downloaded, if not, do it
if (!file.exists("data/vg250-ew_12-31.utm32s.shape.ebenen/vg250-ew_ebenen_1231/VG250_KRS.shp")) {
  download.file(
    "https://daten.gdz.bkg.bund.de/produkte/vg/vg250-ew_ebenen_1231/aktuell/vg250-ew_12-31.utm32s.shape.ebenen.zip",
    destfile = "data/vg250-ew_12-31.utm32s.shape.ebenen.zip",
    mode = 'wb'
  )
  unzip(zipfile = "data/vg250-ew_12-31.utm32s.shape.ebenen.zip",
        exdir = "data/")
}

# load case data
dat <- read.table("data/RKI_COVID19.csv", header = TRUE, sep = ",")

# load spatial boundaries for kreise (county or district)
kreiseSf <-
  st_read("data/vg250-ew_12-31.utm32s.shape.ebenen/vg250-ew_ebenen_1231/VG250_KRS.shp")

```

# Explore & understand COVID Case Data

```{r}
summary(dat)
```

The COVID 19 dataset is `r`ncol(dat)`attributes/columns wide and`r nrow(dat)\` long.

Next, we extract the Bundesland to Kreis relationship.

## Unit of analysis

```{r}

Landkreis2Land <-
  select(dat, IdLandkreis, Landkreis, IdBundesland, Bundesland) %>%
  group_by(IdLandkreis) %>% summarise(
    Landkreis = first(Landkreis) ,
    IdBundesland = first(IdBundesland),
    Bundesland = first(Bundesland)
  )

Landkreis2Land$IdLandkreis_char <-
  as.character(Landkreis2Land$IdLandkreis)
Landkreis2Land$IdBundesland_char <-
  as.character(Landkreis2Land$IdBundesland)
```

A total of `r`nrow(Landkreis2Land)\` are in our dataset.

## Time dimension

Next, we dig into the timeseries component of the data. Therefore we check and convert the date attribute.

```{r}
head(dat$Meldedatum)
tail(dat$Meldedatum)
```

The date format is in days only. Are there duplicated dates, maybe due to different times?

```{r}
# get unique dates
unique_dates <- length(unique(dat$Refdatum))
# check COVID file date
time_diff <- max(ymd_hms(dat$Refdatum)) - min(ymd_hms(dat$Refdatum))
time_diff
```

The dataset includes `r unique_dates` unique dates. Time range across all records is `r time_diff`. Therefore we can conclude there are no duplicates.

```{r}
# convert to date datatype
dat$Refdatum <- ymd_hms(dat$Refdatum)
dat$Meldedatum <- ymd_hms(dat$Meldedatum)
```

## Cases

Let's have a look at one Landkreis and check how many records exist per day.

```{r}
plot(xtabs( ~ Refdatum, data = dat %>% dplyr::filter(Landkreis == "SK Kiel")),
     xlab = "Time in days (RefDatum",
     ylab = "Amount of events")
```

```{r}
# filter data for Kiel and a specific date with several reportings
dat %>% 
  dplyr::filter(Landkreis == "SK Kiel" & Refdatum == ymd("2020-12-01")) %>%
  head()
```

We see multiple events per day and Landkreis. This is due to further grouping variables like age groups and gender, but also due to lags in reporting cases.

```{r}
dat %>% 
  dplyr::filter(AnzahlFall < 0) %>% 
  head()

```

Negative case reporting at a later date is possible and needs to be accounted for.

# Aggregation

## Date of infection

Aggregate case and death counts over all age groups and gender

```{r }
options(dplyr.summarise.inform=F)
datAgg <- dat %>% group_by(IdLandkreis,  Refdatum) %>%
  summarize(
    IdBundesland = first(IdBundesland),
    sumCount = sum(AnzahlFall, na.rm = TRUE),
    sumDeath = sum(AnzahlTodesfall, na.rm = TRUE),
    Landkreis = first(Landkreis),
    Bundesland = first(Bundesland)
  )
```

```{r}
summary(datAgg)
```

The -1 in Min *sumCount* is due to that the case had been reported at the day before.

```{r}
complete_ts <- length(unique(datAgg$Refdatum)) * length(unique(datAgg$IdLandkreis))
miss_ts <- length(unique(datAgg$Refdatum)) * length(unique(datAgg$IdLandkreis)) -  nrow(datAgg)
```

A complete timeseries would have `r (length(unique(datAgg$Refdatum)) * length(unique(datAgg$IdLandkreis)))` records. Our dataset is `r miss_ts` short, e.g. a district did not report. For those we need to fill in zeros.

```{r}
plot(xtabs(~ Bundesland, datAgg),
     xlab = "Bundesland",
     ylab = "Amount of events")
```

## Complete timeseries

Make a wide tibble where every record represents a date and every column the sum of cases in a Kreis. First sort by date. Then we replace the NAs in the Kreis columns for the dates with no observations. Afterwards we change to a long tibble in which every record is a Kreis and every column is the sum of cases on a date.

```{r}
# wide tibble with dates as records and Landkreise as columns
datAggCases_wideByDistrict <-
  arrange(datAgg, Refdatum, .by_group = TRUE) %>%
  pivot_wider(names_from = IdLandkreis,
              values_from = sumCount,
              id_cols = Refdatum)
```

```{r}
# long tibble with dates and Landkreise as single records and one case column
datAggCases_long <-
  pivot_longer(datAggCases_wideByDistrict,
               cols = -Refdatum,
               names_to = "IdLandkreis")
```

Replace NA with zeros and join the name of Landkreis and Bundesland.

```{r}
# Replace NA with zeros
datAggCases_long$sumCount <-
  replace_na(datAggCases_long$value, replace = 0)
# join long tibble name of Landkreis and Bundesland
datAggCases_long <-
  left_join(datAggCases_long,
            Landkreis2Land,
            by = c("IdLandkreis" = "IdLandkreis_char")) %>%
  dplyr::select(-c(IdLandkreis.y, value)) # remove duplicate attribute and not needed value attr 
```

Visualize cases by date and Bundesland

```{r, fig.width=11, fig.height=12}
ggplot(datAggCases_long, aes(
  x = Refdatum,
  y = sumCount,
  colour = factor(IdLandkreis)
)) +
  geom_line(show.legend = FALSE) + facet_wrap(~ Bundesland) +
  xlab("Time in days") + ylab("Amount of cases") +
  theme_light()
```

## Aggregation to 7 day incidence

In order to minimize noise from daily reporting, we will now aggregate our data to 7 day incidence rolling mean and sum. We also add the R0 reproduction value. The calculation is based on <https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Projekte_RKI/R-Wert-Erlaeuterung.pdf?__blob=publicationFile>

```{r}
datAggCases_long <- datAggCases_long %>% group_by(IdLandkreis) %>%
  arrange(Refdatum) %>%
  summarise(
    sumCountRollMean7 = rollapply(
      sumCount,
      width = 7,
      FUN = mean,
      na.rm = TRUE,
      fill = NA
    ),
    sumCountSum7 = rollapply(
      sumCount,
      width = 7,
      FUN = sum,
      align = "right",
      partial = FALSE,
      fill = NA,
      na.rm = TRUE
    ),
    # first 7day window of i+1:i-5
    sumOverlap1 = rollapply( 
      sumCount,
      width = list(1:-5),
      FUN = sum,
      align = "right",
      partial = FALSE,
      fill = NA,
      na.rm = TRUE
    ),
    # second 7day window of i-3:i-9
    sumOverlap2 = rollapply(
      sumCount,
      width = list(-3:-9),
      FUN = sum,
      align = "right",
      partial = FALSE,
      fill = NA,
      na.rm = TRUE
    ),
    Refdatum = Refdatum
  ) %>%
  mutate(R0_7 = case_when(
    is.infinite(round(sumOverlap1 / sumOverlap2, digits = 4)) == TRUE ~ as.numeric(NA),
    TRUE ~ round(sumOverlap1 / sumOverlap2, digits = 4)
  )) %>%
  left_join(datAggCases_long, by = c("IdLandkreis", "Refdatum")) %>%  
  select(-c(sumOverlap1, sumOverlap2))
```

Visualization as plots

```{r, warning=FALSE,  fig.width=11, fig.height=12}
ggplot(datAggCases_long,
       aes(
         x = Refdatum,
         y = sumCountRollMean7,
         colour = factor(IdLandkreis)
       )) +
  geom_line(show.legend = FALSE) + facet_wrap( ~ Bundesland) + theme_light()

ggplot(datAggCases_long,
       aes(
         x = Refdatum,
         y = sumCountSum7,
         colour = factor(IdLandkreis)
       )) +
  geom_line(show.legend = FALSE) + facet_wrap( ~ Bundesland)

ggplot(datAggCases_long,
       aes(
         x = Refdatum,
         y = R0_7,
         colour = factor(IdLandkreis)
       )) +
  geom_line(show.legend = FALSE) + facet_wrap( ~ Bundesland)
```

### Link to spatial data

Now we want to link the COVID 19 case data onto the spatial boundaries of each Kreis.

```{r}
head(kreiseSf)
```

The attribute ARS from the spatial dataset can be linked to IdLandkreis from the COVID 19 dataset

```{r}
length(unique(kreiseSf$ARS))
length(unique(datAgg$IdLandkreis))
head(unique(kreiseSf$ARS))
head(unique(datAgg$IdLandkreis))
```

We see a difference of 10 Kreise which are not present in the spatial dataset. We keep that in mind. First we change the setup of our COVID 19 tibble to a wide format with every row representing one Kreis and every reporting day as attribute/column. Also we need to add a 0 on every Landkreise Id.

```{r}
datAgg_wide <-
  arrange(datAggCases_long, Refdatum, .by_group = TRUE) %>%
  pivot_wider(
    id_cols = IdLandkreis,
    names_from = Refdatum,
    values_from = c(sumCountSum7),
    names_prefix = "cases7days_"
  )
head(datAgg_wide)
dim(datAgg_wide)
```

Check which columns show NAs only.

```{r}
#apply(datAgg_wide, MARGIN = 2, FUN = function(x) length(which(is.na(x))))
idx_na <- which(is.na(datAgg_wide), arr.ind = TRUE)
unique(names(datAgg_wide)[idx_na[,2]])
#datAgg_wide <- replace_na(datAgg_wide, replace = 0)
```

Only the first couple of days in the dataset are filled with NAs Next we join the case data onto the spatial boundaries.

```{r}
# convert to string format
datAgg_wide$IdLandkreis_char <-
  as.character(datAgg_wide$IdLandkreis)
# add 0 to every Landkreise Id
datAgg_wide$IdLandkreis_char <-
  str_pad(
    datAgg_wide$IdLandkreis_char,
    width = 5,
    side = "left",
    pad = "0"
  )

kreiseSf %>% filter(is.na(match(
  kreiseSf$ARS, datAgg_wide$IdLandkreis_char
)) == T)
```

We see a Problem with Berlin and Eisenach. Eisenach merged with a neighboring Kreis in July 2021.

```{r}
Landkreis2Land %>% filter(str_detect(Landkreis, "Berlin"))
kreiseSf %>% filter(str_detect(GEN, "Berlin"))
```

Several districts for Berlin which are not in the geodata - we need to aggregate the data.

```{r}
idx_berlin <- which(Landkreis2Land$Bundesland == "Berlin")
idx_berlinInRKI <-
  which(datAgg_wide$IdLandkreis %in% Landkreis2Land$IdLandkreis[idx_berlin])

incidenceFieldsIdxRki <-
  grep(pattern = "cases7days_", x = names(datAgg_wide))

# add the data as a new row
n <- nrow(datAgg_wide)

sumCases4Berlin <-
  apply(
    datAgg_wide[idx_berlinInRKI, incidenceFieldsIdxRki],
    MARGIN = 2,
    FUN = function(x)
      sum(x, na.rm = TRUE)
  )
datAgg_wide[n + 1, ] <- rep(NA, n = ncol(datAgg_wide))
datAgg_wide[n + 1, 1] <-
  "11000" #c(11000, t(as.numeric(sumCases4Berlin)), "11000")
datAgg_wide[n + 1, 2:(ncol(datAgg_wide) - 1)] <-
  t(as.numeric(sumCases4Berlin))
datAgg_wide[n + 1, ncol(datAgg_wide)] <- "11000"

# drop the bezirke of berlin from the tibble to avoid double counting later on
datAgg_wide <- datAgg_wide[-idx_berlinInRKI, ]
```

Add Berlin to the Landkreis- Bundesland relationship table as well.

```{r}
Landkreis2Land[nrow(Landkreis2Land)+1, ] <- list(IdLandkreis = 11000, Landkreis = "Berlin", Bundesland = "Berlin", IdLandkreis_char = "11000")
```

```{r}
kreiseWithCovid <- left_join(kreiseSf, datAgg_wide, by= c("ARS" = "IdLandkreis_char"))

# ensure proper field names
incidenceFieldsIdxKreise <-
  grep(pattern = "cases7days_", x = names(kreiseWithCovid))

names(kreiseWithCovid)[incidenceFieldsIdxKreise] <-
  gsub(pattern = "-",
       replacement = "_",
       names(kreiseWithCovid)[incidenceFieldsIdxKreise])
```

### Test maps

Districts without inhabitants

```{r}
#idx0ew <- which(kreiseWithCovid$EWZ == 0)
kreiseWithCovid %>% filter( EWZ == 0) %>% select(ADE:WSK)

kreiseWithCovid$noEWZ <- ifelse(kreiseWithCovid$EWZ == 0, 1, 0)
```

```{r}
kreiseWithCovid %>% filter(EWZ == 0) %>%
  tm_shape() + tm_polygons(col = "noEWZ", n = 2) +
  tm_layout(legend.outside = TRUE)
```

These are all GF==2 areas, i.e. water bodies and other uninhabited regions.

Drop those

```{r}
kreiseWithCovidCleaned <- filter(kreiseWithCovid, GF != 2)
```

Lets do a map of the incidence rate

```{r}
breaks = c(0, 5, 25, 50, 100, 250, 500, 1000)
kreiseWithCovidCleaned %>% mutate(incidence_rate = cases7days_2021_09_27 / EWZ * 10 ^
                                    5) %>%
  tm_shape() + tm_polygons(col = "incidence_rate", breaks = breaks) +
  tm_layout(legend.outside = TRUE)
```

We see one unit with a missing incidence rate. This is the boundary of the former Kreis Eisenach. Since July 2021 it is merged with the surrounding Kreis.

## Aggregate by Meldedatum

Actually these seem to be the numbers in the official maps provided by the RKI. Lets redo the steps but for the Meldedatum attribute.

```{r}
datAggMelde <- dat %>% group_by(IdLandkreis,  Meldedatum) %>%
  summarize(
    sumCount = sum(AnzahlFall, na.rm = TRUE),
    sumDeath = sum(AnzahlTodesfall, na.rm = TRUE),
    Landkreis = first(Landkreis) ,
    Bundesland = first(Bundesland)
  )
```

```{r}
datAggCasesMelde_wideByDistrict <-
  arrange(datAggMelde, Meldedatum, .by_group = TRUE) %>%
  pivot_wider(names_from = IdLandkreis,
              values_from = sumCount,
              id_cols = Meldedatum)
```

```{r}
datAggCasesMelde_long <-
  pivot_longer(datAggCasesMelde_wideByDistrict,
               cols = -Meldedatum,
               names_to = "IdLandkreis")
```

Replace NA with zeros

```{r}
datAggCasesMelde_long$sumCount <-
  replace_na(datAggCasesMelde_long$value, replace = 0)
datAggCasesMelde_long <-
  left_join(datAggCasesMelde_long,
            Landkreis2Land,
            by = c("IdLandkreis" = "IdLandkreis_char"))
```

```{r, fig.width=11, fig.height=12}
ggplot(datAggCasesMelde_long,
       aes(
         x = Meldedatum,
         y = sumCount,
         colour = factor(IdLandkreis)
       )) +
  geom_line(show.legend = FALSE) + facet_wrap( ~ Bundesland)
```

## 7-day rolling mean and 7-day sum

```{r}
datAggCasesMelde_long <-
  datAggCasesMelde_long %>% group_by(IdLandkreis) %>%
  summarise(
    sumCountRollMean7 = rollapply(
      sumCount,
      width = 7,
      FUN = mean,
      na.rm = TRUE,
      fill = NA
    ),
    sumCountSum7 = rollapply(
      sumCount,
      width = 7,
      FUN = sum,
      align = "right",
      partial = FALSE,
      fill = NA,
      na.rm = TRUE
    ),
    # first 7day window of i+1:i-5
    sumOverlap1 = rollapply( 
      sumCount,
      width = list(1:-5),
      FUN = sum,
      align = "right",
      partial = FALSE,
      fill = NA,
      na.rm = TRUE
    ),
    # second 7day window of i-3:i-9
    sumOverlap2 = rollapply(
      sumCount,
      width = list(-3:-9),
      FUN = sum,
      align = "right",
      partial = FALSE,
      fill = NA,
      na.rm = TRUE
    ),
    Meldedatum = Meldedatum
  ) %>%
  mutate(R0_7 = case_when(
    is.infinite(round(sumOverlap1 / sumOverlap2, digits = 4)) == TRUE ~ as.numeric(NA),
    TRUE ~ round(sumOverlap1 / sumOverlap2, digits = 4)
  )) %>%
  left_join(datAggCasesMelde_long, by = c("IdLandkreis", "Meldedatum")) %>% 
  select(-c(sumOverlap1, sumOverlap2))
```

### Join spatial data

Bring to wide format

```{r}
datAggMelde_wide <-
  arrange(datAggCasesMelde_long, Meldedatum, .by_group = TRUE) %>%
  pivot_wider(
    id_cols = IdLandkreis,
    names_from = Meldedatum,
    values_from = sumCountSum7,
    names_prefix = "cases7days_"
  )
```

```{r}
# convert to string format
datAggMelde_wide$IdLandkreis_char <-
  as.character(datAggMelde_wide$IdLandkreis)
# add 0 to every Landkreise Id
datAggMelde_wide$IdLandkreis_char <-
  str_pad(
    datAggMelde_wide$IdLandkreis_char,
    width = 5,
    side = "left",
    pad = "0"
  )

kreiseSf %>% filter(is.na(match(
  kreiseSf$ARS, datAgg_wide$IdLandkreis_char
)) == T)
```

We see a Problem with Berlin and Eisenach. Eisenach merged with a neighboring Kreis in July 2021.

Several districts for Berlin which are not in the geodata - need to aggregate the data.

```{r}
idx_berlin <- which(Landkreis2Land$Bundesland == "Berlin")
idx_berlinInRKI <-
  which(datAggMelde_wide$IdLandkreis %in% Landkreis2Land$IdLandkreis[idx_berlin])

incidenceFieldsIdxRki <-
  grep(pattern = "cases7days_", x = names(datAggMelde_wide))

# add the data as a new row
n <- nrow(datAggMelde_wide)

sumCases4Berlin <-
  apply(
    datAggMelde_wide[idx_berlinInRKI, incidenceFieldsIdxRki],
    MARGIN = 2,
    FUN = function(x)
      sum(x, na.rm = TRUE)
  )
datAggMelde_wide[n + 1, ] <- rep(NA, n = ncol(datAggMelde_wide))
datAggMelde_wide[n + 1, 1] <-
  "11000" #c(11000, t(as.numeric(sumCases4Berlin)), "11000")
datAggMelde_wide[n + 1, 2:(ncol(datAggMelde_wide) - 1)] <-
  t(as.numeric(sumCases4Berlin))
datAggMelde_wide[n + 1, ncol(datAggMelde_wide)] <- "11000"

# drop the bezirke of berlin from the tibble to avoid double counting later on
datAggMelde_wide <- datAggMelde_wide[-idx_berlinInRKI, ]
```

```{r}
kreiseWithCovidMelde <-
  left_join(kreiseSf, datAggMelde_wide, by = c("ARS" = "IdLandkreis_char"))

# ensure proper field names
incidenceFieldsIdxKreise <-
  grep(pattern = "cases7days_", x = names(kreiseWithCovidMelde))
names(kreiseWithCovidMelde)[incidenceFieldsIdxKreise] <-
  gsub(pattern = "-",
       replacement = "_",
       names(kreiseWithCovidMelde)[incidenceFieldsIdxKreise])
```

Remove waterbodies again.

```{r}
kreiseWithCovidMeldeCleaned <- filter(kreiseWithCovidMelde, GF != 2)
```

```{r}
breaks = c(0, 5, 25, 50, 100, 250, 500, 1000)
kreiseWithCovidMeldeCleaned %>% mutate(incidence_rate = cases7days_2021_09_27 / EWZ * 10 ^
                                         5) %>%
  tm_shape() + tm_polygons(
    col = "incidence_rate",
    breaks = breaks,
    legend.hist = TRUE,
    palette = "-plasma",
    legend.reverse = TRUE
  ) +
  tm_layout(
    legend.outside = TRUE,
    bg.color = "darkgrey",
    outer.bg.color = "lightgrey",
    legend.outside.size = 0.5,
    attr.outside = TRUE
  ) + tm_scale_bar()
```

## Aggregate for each calender week

```{r}
datAggCasesMeldeWeekly_long <-
  datAggCasesMelde_long %>% group_by(IdLandkreis) %>% tq_transmute(
    select = c("sumCount"),
    mutate_fun = apply.weekly,
    FUN        = sum,
    na.rm      = TRUE,
    col_rename = "sumCount_weekly"
  )
```

```{r}
datAggMeldeWeekly_wide <-
  arrange(datAggCasesMeldeWeekly_long, Meldedatum, .by_group = TRUE) %>%
  pivot_wider(
    id_cols = IdLandkreis,
    names_from = Meldedatum,
    values_from = sumCount_weekly,
    names_prefix = "casesWeek_"
  )
```

```{r}
datAggMeldeWeekly_wide$IdLandkreis_char <-
  as.character(datAggMeldeWeekly_wide$IdLandkreis)
datAggMeldeWeekly_wide$IdLandkreis_char <-
  str_pad(
    datAggMeldeWeekly_wide$IdLandkreis_char,
    width = 5,
    side = "left",
    pad = "0"
  )
```

Several districts for Berlin which are not in the geodata - need to aggregate the data.

```{r}
idx_berlin <- which(Landkreis2Land$Bundesland == "Berlin")
idx_berlinInRKI <-
  which(datAggMeldeWeekly_wide$IdLandkreis %in% Landkreis2Land$IdLandkreis[idx_berlin])

incidenceFieldsIdxRki <-
  grep(pattern = "casesWeek_", x = names(datAggMeldeWeekly_wide))

# add the data as a new row
n <- nrow(datAggMeldeWeekly_wide)

sumCases4Berlin <-
  apply(
    datAggMeldeWeekly_wide[idx_berlinInRKI, incidenceFieldsIdxRki],
    MARGIN = 2,
    FUN = function(x)
      sum(x, na.rm = TRUE)
  )
datAggMeldeWeekly_wide[n + 1, ] <-
  rep(NA, n = ncol(datAggMeldeWeekly_wide))
datAggMeldeWeekly_wide[n + 1, 1] <-
  "11000" #c(11000, t(as.numeric(sumCases4Berlin)), "11000")
datAggMeldeWeekly_wide[n + 1, 2:(ncol(datAggMeldeWeekly_wide) - 1)] <-
  t(as.numeric(sumCases4Berlin))
datAggMeldeWeekly_wide[n + 1, ncol(datAggMeldeWeekly_wide)] <-
  "11000"

# drop the bezirke of berlin from the tibble to avoid double counting later on
datAggMeldeWeekly_wide <- datAggMeldeWeekly_wide[-idx_berlinInRKI, ]
```

### Join spatial data

```{r}
kreiseWithCovidMeldeWeekly <-
  left_join(kreiseSf,
            datAggMeldeWeekly_wide,
            by = c("ARS" = "IdLandkreis_char"))

# ensure proper field names
incidenceFieldsIdxKreise <-
  grep(pattern = "casesWeek_",
       x = names(kreiseWithCovidMeldeWeekly))
names(kreiseWithCovidMeldeWeekly)[incidenceFieldsIdxKreise] <-
  gsub(pattern = "-",
       replacement = "_",
       names(kreiseWithCovidMeldeWeekly)[incidenceFieldsIdxKreise])
```

```{r}
kreiseWithCovidMeldeWeeklyCleaned <-
  filter(kreiseWithCovidMeldeWeekly, GF != 2)
```

```{r}
breaks = c(0, 5, 25, 50, 100, 250, 500, 1000)
kreiseWithCovidMeldeWeeklyCleaned %>% mutate(incidence_rate = casesWeek_2021_09_26 / EWZ * 10 ^
                                               5) %>%
  tm_shape() + tm_polygons(
    col = "incidence_rate",
    breaks = breaks,
    legend.hist = TRUE,
    palette = "-plasma",
    legend.reverse = TRUE
  ) +
  tm_layout(
    legend.outside = TRUE,
    bg.color = "darkgrey",
    outer.bg.color = "lightgrey",
    legend.outside.size = 0.5,
    attr.outside = TRUE
  ) + tm_scale_bar()
```

```{r}
save(kreiseWithCovidMeldeCleaned, file = "data/kreiseWithCovidByMeldedatum.Rdata")
save(kreiseWithCovidCleaned, file = "data/kreiseWithCovidByRefdatum.Rdata")
save(kreiseWithCovidMeldeWeeklyCleaned, file = "data/kreiseWithCovidMeldedatumWeekly.Rdata")
```

```{r}
st_write(
  kreiseWithCovidMeldeWeeklyCleaned,
  dsn = "data/covidKreise.gpkg",
  layer = "kreiseWithCovidMeldeWeeklyCleaned",
  delete_layer = TRUE
)
```


# Appendix

## Berlin Bezirke

### Spatial reference

Download boudnary data for the Berlin Bezirke

```{r}
# Berlin city districts

# interact with OWS API to list available data and put together the correct request
#wfs_berlin <- "https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_ortsteile"
#berlin_client <- WFSClient$new(wfs_berlin, serviceVersion = "2.0.0")
#berlin_client$getFeatureTypes(pretty = TRUE)

#url <- httr::parse_url(wfs_berlin)

#url$query <- list(service = "wfs",
#                  version = "2.0.0", # optional
#                  request = "GetFeature",
#                  typenames = "fis:s_wfs_alkis_ortsteile",
#                  srsName = "EPSG:25833"
#                  )
#request <-httr::build_url(url)
#berSkSf <- st_read(request)

berBzkSf <-
  st_read(
    "https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_wfs_alkis_ortsteile?service=wfs&version=2.0.0&request=GetFeature&typenames=fis%3As_wfs_alkis_ortsteile&srsName=EPSG%3A25833",
    quiet = T
  )
```


In the following code-chunk we check the consistency of our spatial dataset from 


```{r warning=F}
# Aggregate to Bezirke
berSkSf <- berBzkSf %>%
  mutate(IdBerlinSK = paste0(substr(sch, 1, 2), substr(sch, 6, 8))) %>%
  group_by(IdBerlinSK) %>%
  summarise() #union boundaries

# Filter non Berlin districts
kreise <- kreiseSf %>% filter(GEN != "Berlin") %>% select(c(ARS, EWZ))
# Reproject to consistent coord system
kreise <- kreise %>% st_transform(st_crs(kreiseSf))
berSkSf <- berSkSf %>% st_transform(st_crs(kreiseSf))
# Filter Districts that are close to Berlin
berProximity <- kreise %>% filter(
  st_intersects(
    kreiseSf %>% filter(GEN == "Berlin") %>% summarise() %>% st_buffer(500),
    geometry,
    sparse = FALSE,
    prepared =
      TRUE
  )
)

plot(berSkSf$geometry, col = sf.colors(categorical = TRUE, alpha = .5))
#plot(berSkSf$geometry)
plot(berProximity, add = T)

plot(berSkSf %>% filter(IdBerlinSK == "11005") %>% select(geometry))
plot(kreise$geom,
     border = "red",
     add = T,
     lwd = 1)
```

We see that the data does not seamingless fit. We prefer to avoid topological errors that could arise from false gaps between the polygons which could later affect the neighboordhod estimation. In the following we extent every polygon of Berlin and subtract it with all others including the surrounding polygons of Brandenburg.

```{r warning = F}
# loop over evey Bezirk of Berlin
for (i in 1:nrow(berSkSf)) {
  
  # extend/buffer with 300m 
  berBuff <- berSkSf[i,] %>% st_buffer(300)
  
  # Subtract/difference all other Berlin Bezirke
  berBuffDiff <- berBuff %>% st_difference(st_union(berSkSf[-i,]))
  
  # Subtract/difference all other Kreise/Districts outside of Berlin
  berBuffDiff <- berBuffDiff %>% st_difference(st_union(berProximity))
  
  # check for first iteration
  if (i==1){
    # store result in different variable
    berSkSf_fit <- berBuffDiff
  } else {
    # append variable by the new result
    berSkSf_fit <- rbind(berSkSf_fit, berBuffDiff)
  }
}
# homogenize attributes



# Popualtion per Berlin bezirk from https://de.wikipedia.org/wiki/Verwaltungsgliederung_Berlins
berlin_EWZ <- data.frame(
  ARS = c(
    11001,
    11002,
    11003,
    11004,
    11005,
    11006,
    11007,
    11008,
    11009,
    11010,
    11011,
    11012
  ),
  EWZ = c(
    383360,
    289787,
    410716,
    341392,
    245527,
    308840,
    349539,
    327945,
    276165,
    273731,
    296837,
    266123
  )
)
# Alter names in the Blerin Bezirke dataset to be consistent
names(berSkSf_fit) <- c("ARS", "geometry")
# Add population data to the Bezirke (just in ascending order, not a join)
berSkSf_fit$EWZ <- berlin_EWZ$EWZ
# Bind the rows of the Bezirke and Kreise in one final dataset
kreise <- rbind(berSkSf_fit, kreise)
# check the borders again
plot(berSkSf_fit %>% filter(ARS == "11005") %>% select(geometry), lwd=1)
plot(kreise$geometry,
     lwd = .5,
     border = "red",
     add = T
     )
# map on pop distribution
plot(kreise["EWZ"], lwd=.01)
```


### Socioeconomic data for Berlin Bezirke

```{r}
# get employment data
if (!file.exists("data/erwerbstaetigkeit-regionaldaten-2019-erwerbstaetige.xlsx")) {
  download.file(
    "https://download.statistik-berlin-brandenburg.de/15c8291864bb541b/db8a520a733e/erwerbstaetigkeit-regionaldaten-2019-erwerbstaetige.xlsx",
    destfile = "data/erwerbstaetigkeit-regionaldaten-2019-erwerbstaetige.xlsx",
    mode = 'wb'
  )
}
# load in employment data. The format is a bit ugly. the header spans over 4 rows and the 6th column is empty.
berlin_work <- readxl::read_xlsx("data/erwerbstaetigkeit-regionaldaten-2019-erwerbstaetige.xlsx", skip=4, n_max = 10)
# rm column 6, it is empty
berlin_work <- berlin_work[,-6]
# add correct collumn names
names(berlin_work) <- c("Bezirk", "Pop", "Employmed", "Unemployed", "Non-employed", "Employmentrate")

# download demographic data
if (!file.exists("data/EWR_Ortsteile_2020-12-31.csv")) {
  download.file(
    "https://download.statistik-berlin-brandenburg.de/01e6d9b311a4b497/0836b0ac615a/EWR_Ortsteile_2020-12-31.csv",
    destfile = "data/EWR_Ortsteile_2020-12-31.csv",
    mode = 'wb'
  )
}

berlin_demo <-
  read.table(
    "data/EWR_Ortsteile_2020-12-31.csv",
    sep = ";",
    header = T,
    fileEncoding = "ISO 8859-14",
    #encoding = "ISO 8859-14"
    #encoding = "UTF-8"
  ) %>% tibble()

berlin_demo <- berlin_demo %>%
  group_by(Bez.Name, Geschl, Staatsangeh, Altersgr) %>%
  summarise(Bezirk = first(Bezirk),
            Häufigkeit = sum(Häufigkeit))

```
